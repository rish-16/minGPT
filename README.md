# min-GPT x AFT-Full

A test to check the performance of the AFT-Full layer from the paper "An Attention Free Transformer" by Zhai et al. from Apply, Inc.. All credits to Andrej Karpathy for the implementation of min-GPT.

This repo is purely experimental to check if the loss decreases when GPT is used for image generation (Ã  la Image-GPT) when trained on CIFAR-10.